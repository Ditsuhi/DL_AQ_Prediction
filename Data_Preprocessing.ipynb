{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqdd1k70Dk2dBP4aYj3aWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ditsuhi/Nitrogen_Dioxide_Prediction/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import all required libraries\n",
        "\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.interpolate import NearestNDInterpolator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import ConvLSTM2D, BatchNormalization\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import  Conv2D \n",
        "from time import time\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "tdMCCoKA6PpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jREIAZxGuDsH"
      },
      "outputs": [],
      "source": [
        "# To calculate nearest neighbor interpolation for meteorological data\n",
        "\n",
        "def CalcNNvalue(array_interpolate):\n",
        "  \n",
        "  array_float = array_interpolate.astype(float)\n",
        "  knowncell_position= np.argwhere(array_float!=0)  \n",
        "  knowncell_value = array_float[array_float!=0] \n",
        "  unknowncell_position = np.argwhere(array_float==0)\n",
        "  myInterpolator = NearestNDInterpolator(knowncell_position, knowncell_value) \n",
        "  unknown_values = myInterpolator(unknowncell_position)\n",
        "  array_float[array_float == 0 ] = unknown_values\n",
        "  return array_float.tolist()\n",
        "\n",
        "\n",
        "def calc_NN_fullData(full_data):\n",
        "  NN_list =[]\n",
        "  for item in full_data:    \n",
        "    try: \n",
        "      NN_list.append(CalcNNvalue(item))\n",
        "    except IndexError:\n",
        "      NN_list.append(item.tolist())  \n",
        "  return NN_list\n",
        "\n",
        "\n",
        "def calculate_NN_fullData_allAttributes (df_all):\n",
        "  df_all_NN_list = []\n",
        "  # The number in the range is the number of meteorological features \n",
        "  # to be interpolated using nearest neighbor interpolation.  \n",
        "  for attr_numb in range(1): \n",
        "    certain_attr = df_all[:, :, attr_numb]    \n",
        "    certain_attr_reshaped= certain_attr.reshape(certain_attr.shape[0], 20, 17)\n",
        "    certain_attr_reshaped_NN = calc_NN_fullData(certain_attr_reshaped) \n",
        "    certain_attr_reshaped_NN_original_shape = np.reshape(certain_attr_reshaped_NN, (certain_attr.shape[0], 340))\n",
        "    df_all_NN_list.append(certain_attr_reshaped_NN_original_shape.tolist())   \n",
        "  df_all_NN_array = np.dstack((item)for item in df_all_NN_list)  \n",
        "  return df_all_NN_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip data giving the path of certain dataset\n",
        "\n",
        "path = '/content/AirMetTraffic_2019_2020_firstSixMonths.zip'\n",
        "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "airMetTraf = glob(\"/content/*.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N06QOh1fugjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort dataset in chronological order\n",
        "\n",
        "\n",
        "def sortingFiles(eachFile):\n",
        "    return int(eachFile) if eachFile.isdigit() else eachFile\n",
        "def natural_keys(eachFile):\n",
        "    return [sortingFiles(c) for c in re.split('(\\d+)',eachFile)]\n",
        "\n",
        "sorted_airMetTraf= sorted(airMetTraf, key = natural_keys)\n",
        "sorted_airMetTraf_2019 = sorted_airMetTraf[:4344]\n",
        "sorted_airMetTraf_2020= sorted_airMetTraf[4344:]"
      ],
      "metadata": {
        "id": "r7RyhkxauoKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the feyures from the matrices: FID\t NO2\t UV\t windSpeed\t windDir\t Temp\t Humidity\t Pressure\t SolarRad\t Prec\t intensidad\t ocupacion\t carga\t vmed\n",
        "\n",
        "df_2019 = [pd.read_csv(f, usecols=[' NO2', ' UV',  ' windSpeed', ' windDir', ' Temp', ' Humidity', ' Pressure', ' SolarRad', ' Prec', ' intensidad',\t' ocupacion',\t' carga',\t ' vmed']).values for f in sorted_airMetTraf_2019]\n",
        "#df_2020 = [pd.read_csv(f, usecols=[' NO2', ' UV',  ' windSpeed', ' windDir', ' Temp', ' Humidity', ' Pressure', ' SolarRad', ' Prec', ' intensidad',\t' ocupacion',\t' carga',\t ' vmed']).values for f in sorted_airMetTraf_2020]"
      ],
      "metadata": {
        "id": "bsXY4bOdu77k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_2019  = np.asarray(df_2019)\n",
        "#df_all_2020  = np.asarray(df_2020)"
      ],
      "metadata": {
        "id": "ob6shDDXwKIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This step is for outlier handling (Temperature:res; Humidity:reshum;\n",
        "# and Average Speed:speed)\n",
        "\n",
        "def tempOut (df_all):\n",
        "  return np.where(df_all[:, :, 4] < -3)\n",
        "\n",
        "def humOut (df_all):\n",
        "  return np.where(df_all[:, :, 5] < 0)\n",
        "\n",
        "def speedOut (df_all):\n",
        "  return np.where(df_all[:, :, 12] < 0)\n",
        "\n",
        "res = tempOut (df_all_2019)\n",
        "reshum = humOut (df_all_2019)\n",
        "speed = speedOut (df_all_2019)\n",
        "\n",
        "\n",
        "# all values for a temperature data below -3 are converted to an average\n",
        "# before and after the values.\n",
        "\n",
        "for i in range(len(res[0])):\n",
        "  if df_all[:, :, 4][res[0][i]][res[1][i]-1] > -3 and df_all[:, :, 4][res[0][i]][res[1][i]+1] > -3:\n",
        "    df_all[:, :, 4][res[0][i]][res[1][i]] = (df_all[:, :, 4][res[0][i]][res[1][i]-1]+df_all[:, :, 4][res[0][i]][res[1][i]+1])/2\n",
        "\n",
        "\n",
        "# all values for a humidity data below 0 are converted to an average\n",
        "# before and after the values.\n",
        "\n",
        "for i in range(len(reshum[0])):\n",
        "  if df_all[:, :, 5][reshum[0][i]][reshum[1][i]-1] >= 0 and df_all[:, :, 5][reshum[0][i]][reshum[1][i]+1] >= 0:\n",
        "    df_all[:, :, 5][reshum[0][i]][reshum[1][i]] = (df_all[:, :, 5][reshum[0][i]][reshum[1][i]-1]+df_all[:, :, 5][reshum[0][i]][reshum[1][i]+1])/2\n",
        "\n",
        "\n",
        "# all values for a speed data below 0 are converted to 0.\n",
        "\n",
        "for i in range(len(speed[0])):\n",
        "  df_all[:, :, 12][speed[0][i]][speed[1][i]] = 0\n",
        "\n",
        "\n",
        "# deleting precipitation, because most values are 0\n",
        "\n",
        "df_all_non_prec = np.delete(df_all, 8, 2)\n",
        "air= df_all_non_prec[:, :, 0].reshape(-1, 340, 1)\n",
        "traf =  df_all_non_prec[:, :, 8:12].reshape(-1, 340, 4)\n",
        "NN_dataframe = calculate_NN_fullData_allAttributes (df_all_non_prec[:, :, 1:8])\n",
        "df_air_NN_Met = np.concatenate((air, idw_dataframe, traf), axis=2)\n",
        "not_nun = np.nan_to_num(df_air_NN_Met)\n",
        "round_data = np.round(not_nun, 1)"
      ],
      "metadata": {
        "id": "pg8vhfF9vhSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert wind direction to categorical data, and then apply One Hot Encoder\n",
        "\n",
        "df_categ = pd.DataFrame(round_data.reshape(-1, 12), columns = ['NO2', 'UV',  'windSpeed', 'windDir', 'Temp', 'Humidity', 'Pressure', 'SolarRad',  'intensidad',\t'ocupacion',\t'carga',\t 'vmed'])\n",
        "\n",
        "df_categ['windDir_Categ'] = \"\"\n",
        "\n",
        "for item in range(0, len(df_categ)):\n",
        "  if (df_categ['windDir'][item] >=0 and  df_categ['windDir'][item] <22.5) or df_categ['windDir'][item] >337.5:\n",
        "    df_categ['windDir_Categ'][item] = 'north'        \n",
        "  elif df_categ['windDir'][item] >=22.5 and  df_categ['windDir'][item] < 67.5:\n",
        "    df_categ['windDir_Categ'][item] = 'northeast' \n",
        "  elif df_categ['windDir'][item] >=67.5 and  df_categ['windDir'][item] < 112.5:\n",
        "    df_categ['windDir_Categ'][item] = 'east' \n",
        "  elif df_categ['windDir'][item] >=112.5 and  df_categ['windDir'][item] < 157.5:\n",
        "    df_categ['windDir_Categ'][item] = 'southeast' \n",
        "  elif df_categ['windDir'][item] >=157.5 and  df_categ['windDir'][item] < 202.5:\n",
        "    df_categ['windDir_Categ'][item] = 'south' \n",
        "  elif df_categ['windDir'][item] >=202.5 and  df_categ['windDir'][item] < 247.5:\n",
        "    df_categ['windDir_Categ'][item] = 'southwest' \n",
        "  elif df_categ['windDir'][item] >=247.5 and  df_categ['windDir'][item] < 292.5:\n",
        "    df_categ['windDir_Categ'][item] = 'west' \n",
        "  else: \n",
        "    df_categ['windDir_Categ'][item] = 'northwest' \n"
      ],
      "metadata": {
        "id": "OzX4KiJux9fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=OneHotEncoder(sparse=False)\n",
        "df_categ_encoded = pd.DataFrame(encoder.fit_transform(df_categ[['windDir_Categ']]))\n",
        "df_categ_encoded.columns = encoder.get_feature_names(['windDir_Categ'])\n",
        "df_categ.drop(['windDir_Categ'] ,axis=1, inplace=True)\n",
        "OH_X_train= pd.concat([df_categ, df_categ_encoded ], axis=1)\n",
        "\n",
        "#create final dataset for further analyses by deleting windir, as it is aready converted to caregorical data; deleting UV as it is not available for June 2019 and for whole period of 2020;  \n",
        "#deleting carga(Traffic load-according to the definition is the combination of intensity, occupancy time and capacity of the road),\n",
        "# and vmed(average traffic speed - because it is available only for M30 road which is 15.8% of the case study)\n",
        "\n",
        "final_dataframe = OH_X_train.drop(['windDir', 'UV', 'carga', 'vmed'], axis = 1)"
      ],
      "metadata": {
        "id": "DifHAcVQ6Ont"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xVi_sxfhynlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SE6_cGufySr9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}